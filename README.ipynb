{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mice Brain Mri Segmentation #\n",
    "(actually, it can segment humans' scans as well. the repos' name is a bit stupid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview ##\n",
    "* These scripts allow to finenly segment raster images (MRI scans) using svg images (from histological brain atlas).\n",
    "* Atlas svg images and some other assets are retrieved from Allen Brain Institute web-service, which is free for academic and personal use.\n",
    "    * [Allen Brain Institute website](https://portal.brain-map.org/)\n",
    "    * [Mouse | Human brain atlases](http://atlas.brain-map.org/)\n",
    "    * [Allen Brain Institute API help](http://help.brain-map.org/display/api/Allen+Brain+Atlas+API)\n",
    "* This repository contains a collection of cohesive scripts, **not a Python package or an application**. One should know Python to use this.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook will guide through one through the path to replicate our work:\n",
    "1. [installing nesessary programs](#1.-Installations)\n",
    "1. [creating folders](#2a.-creating-folders) and nessesary files\n",
    "1. [manually segmenting the brains from the scans](#3.-manual-scan-segmentation)\n",
    "1. [preprocessing scans before fine segmentation](#4.-preprocessing-scans)\n",
    "1. [bulk-downloading atlas images](#5a.-downloading-atlas-images) and generating raster masks from them\n",
    "1. [fine-segmenting brains into anatomical regions](#fine-anatomical-segmentation)\n",
    "1. [postprocessing segmentation results](#aftermath)\n",
    "\n",
    "There are also some [comments about design of these scripts](#comments-about-design) in the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installations\n",
    "1. clone this repository\n",
    "1. install [Anaconda] package manager\n",
    "1. [create virtual environment from the file] named `conda_env.yml` contained in this repository\n",
    "1. install [Inkscape], install [ImageJ]\n",
    "1. it is best to install [PyCharm] for code editing, and [set up PyCharm to use Anaconda virtual environment]\n",
    "\n",
    "[Anaconda]: https://www.anaconda.com/products/individual\n",
    "[create virtual environment from the file]: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-from-an-environment-yml-file\n",
    "[PyCharm]: https://www.jetbrains.com/pycharm/download/\n",
    "[set up PyCharm to use Anaconda virtual environment]: https://www.jetbrains.com/help/pycharm/conda-support-creating-conda-virtual-environment.html\n",
    "[Inkscape]: https://inkscape.org/\n",
    "[ImageJ]: https://imagej.nih.gov/ij/download.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "sys.path.append('.')\n",
    "from lib.core import info_classes as IC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. creating folders ##\n",
    "classes `IC.ImageFolderInfo`, `IC.OntologyFolderInfo`, `IC.SegmentationResultsFolderInfo` contain `.write()` method that allows to conveniently create these folders in the filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = pathlib.Path('c:/users/user/example')\n",
    "working_dir.mkdir()\n",
    "\n",
    "image_f= IC.ImageFolderInfo(working_dir/'img')\n",
    "image_f.write()\n",
    "\n",
    "ontology_f = IC.OntologyFolderInfo(working_dir/'ont')\n",
    "ontology_f.write()\n",
    "\n",
    "result_f = IC.SegmentationResultsFolderInfo(image_f, ontology_f, folder=working_dir/'segm')\n",
    "result_f.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. putting files into image folder ##\n",
    "* `lib/default_specifications` folder has configuration files that are copied by the `.write()` methods of info classes into the directory that they create. These are YAML files, that can be used to correct the `lib.pipelines` modules' behaviour.\n",
    "* `image_folder_specification.yml` that was is created in `c:/users/user/example/img` must be edited.\n",
    "* under the `file_name_fields` key there is a list of strings, that compose any file name that is related to image. one should give these fields names that correspond to current experiment parameters, like time, mice strain, etc.\n",
    "    > e.g. if specification looks like this:\n",
    "    file_name_fields:\n",
    "        - a\n",
    "        - b\n",
    "        - c\n",
    "    then image file names are assumed to look like `{a}_{b}_{c}.npy`, and metadata names like `{a}_{b}_{c}.yml`. these names will be broken down by some scripts into \"dictionaries\" of {field_name: field_value}. For instance, name 10_20_xyz.npz will correspond to {\"a\":\"10\", \"b\":\"20\", \"c\":\"xyz\"}.  **It is possible to adjust cropped scan resolution based on the value of \"frame\" field through editing `cropped_image_shapes` in the image folder specification.**\n",
    "* After that transfer the images into the `img_raw` subfolder of image folder, rename them according to the fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. manual scan segmentation ##\n",
    "* manual scan segmentation is done by ImageJ program.\n",
    "* the result of the segmentation for each image is couple files that contain digits separated by tabulations and newlines (ImageJ scripts do not have sophisticated tools for file parsing and formatting). These files are collected by a python script into a single YAML metadata file, based on the values under `metadata keys` in the image folder specification.\n",
    "* to segment images do the following:\n",
    "    1. make list of file names in `img_raw` subfolder, put it in the `ij_img_list.txt` file in the image folder, separated by newlines (only names, not the full paths). Create `ij_pointer.txt`, put 0 into it. there already exist a utility function for this:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import miscellaneous_utils as MU\n",
    "MU.list_raw_images(image_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   \n",
    "   2. Open the `lib/imagej_scripts/brain_segmentation_with_imagej.txt`, edit command for image opening according to [ImageJ macro guide] and [ImageJ macro functions list] to open your images (the function below comment `//IMAGE OPENING FUNCTION`). Initially it opens 32bit little-endian single channel 256x256, written as a C-array.\n",
    "   1. Open imagej, load that file as imagej macro: `macros > install > imagej`\n",
    "   1. Press `h` to open log window with the instructuions. follow the instructions.\n",
    "   1. Run collect `collect_image_metadata.main()` (cell below)\n",
    "   >you can find about metadata collection process and how to tune it with specification editing if you run `help(collect_image_metadata.main)`\n",
    "   \n",
    "[ImageJ macro guide]: https://imagej.nih.gov/ij/developer/macro/macros.html#tools\n",
    "[ImageJ macro functions list]: https://imagej.nih.gov/ij/developer/macro/functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.pipelines import collect_image_metadata\n",
    "collect_image_metadata.main(image_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. preprocessing scans ##\n",
    "* Convert images to numpy native array format via `numpy.save()`, put them into `img` subfolder of the image folder.\n",
    "\n",
    "* The pipeline called below uses `cropped_image_shapes` key of the specification and `frame` field value in the image name as well as some metadata keys to crop and resize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.pipelines import crop_images\n",
    "crop_images.main(image_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize image by intensity, we generate reference values and put them into metadata files. Later on you can choose which key to to use for image normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.pipelines import generate_intensity_reference\n",
    "generate_intensity_reference.main(image_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. downloading atlas images ##\n",
    "variables for downloading are stored in `ontology_folder_specification.yml` file inside ontology folder that you've created above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Edit `atlas_id` and `svg_groups` (a table of them can be found [here](http://help.brain-map.org/display/api/Atlas+Drawings+and+Ontologies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.pipelines import download_svgs\n",
    "download_svgs.download_default_ontology(ontology_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Edit the `slice_coordinates` if you know the coordinates for the first slice and the last slice in the atlas. this allows you to download a list of slice ids and zip it with coordinate range using a script in the cell below (the table is just for utility purposes, you can skip this step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_svgs.download_slice_ids_table(ontology_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a very readable (for an xml) `default.xml` tree of structures for the atlas, and probably a tabulation-separated table of slice ids.\n",
    "For each frame value in image names there should be an svg image with the same name. fill the ontology folder specification  `svg_names_and_slice_ids` with pairs of `frame_field_value: slice_id`. run the script from the cell below to bulk-download the atlas vector images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_svgs.download_svgs(ontology_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can and need to manually edit the `.svg` masks with Inkscape, but note that some Inkscape functions can alter all xml attributes of the paths. structures for rendering are found by `structure_id` attributes of the paths, make sure, they are not deleted in the editing process.\n",
    "1. I advise to compute mean of all images for each frame and put it into a corresponding svg (the rendering pipeline ignores visible objects that do not have `structure_id` attribute, so you can leave image there).\n",
    "2. You **must** create a rectangle with xml attribute `structure_id='bbox'`, that will mark the border of the rendered masks (to edit xml attributes of objects from Inkscape, open the 'xml editor' tab.\n",
    "3. It's a good idea to slightly adjust nodes of atlas structures to fit the contours of the brain.\n",
    "4. Note that you can specify an arbitrary function to modify each mask right before the scan segmentation. Therefore you do need to flip/copy/crop the svg masks if only one hemisphere is covered and you need masks for both hemisperes, or otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. rendering raster masks ##\n",
    "For this one needs to edit `inkscape_executable_path` value in the ontology folder's specification. \n",
    "The rendering pipeline uses image folder specification `cropped_image_shapes` values for determining mask sizes.\n",
    "Details about process can be found in `prerender_masks.main` function documentation. This process might take several hours, but is easily parallelized by calling pipeline from different processes for only a part of all frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.pipelines import prerender_masks\n",
    "prerender_masks.main(ontology_f, image_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. fine anatomical segmentation\n",
    "1. You need to specify a tabulation-separated table which contains names of images. Table header specifies, from which columns images fall into 'control' group and from which -- into the 'effect' group. images from the one row constitute a batch and are compared against each other. the `batch_for_comparison` pipeline can create this table. Note that one can easily edit this table in MS Excel/Libre Office, etc.\n",
    "You can find about how to control pipeline behaviour through a result folder specification in the `batch_for_comparison.main` description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.pipelines import batch_for_comparison\n",
    "batch_for_comparison.main(result_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. please read the `compare_and_segment.main` help for detalis on how to control it's behaviour with specification file and some other files, that can be created in the `spec` subfolder of results folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.pipelines import compare_and_segment\n",
    "compare_and_segment.main(result_f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. segmentation results are saved into a temporary folder to save RAM and allow parallel execution (by running several processes and supplying `batch_range` argument to  `compare_and_segment.main`). Because of this design choice, segmentation results' pieces are aggregated by a separate function. You also need to delete temp folder yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_and_segment.collect_segmentation_results(result_f)\n",
    "# cleanup\n",
    "# f = result_f.segmentation_temp()\n",
    "# for e in f.iterdir():\n",
    "#     e.unlink()\n",
    "# f.rmdir()\n",
    "# del e, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. aftermath\n",
    "There are some scripts for processing the results table. Consult with their descriptions to understand what do they do.\n",
    "also there is an execute_all script, that just calls them in a logical order (some scripts depend on each other). They also use specification for the results folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures = (\n",
    "    'Main Olfactory Bulb',\n",
    "    'Olfactory Nerve',\n",
    "    'Anterior Olfactory Nucleus',\n",
    "    'Piriform Area',\n",
    "    'Piriform-Amygdalar Area',\n",
    "    'Cortical Amygdalar Area',\n",
    "    'Entorhinal Area',\n",
    "    'Olfactory Tubercle',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_results_processing.execute_all(srfi, structure_list_for_significance_table=structures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comments about design ##\n",
    "There were several design principles that are used everywhere in these scripts:\n",
    "* There are multiple places in code where one file can be read, but only one place where it's written (with exception of reference generation pipeline, which adds information to metadata.\n",
    "* There is single place where all the information about key concept is aggregated (one of info classes for behavior and specification for details).\n",
    "* Pipelines are isolated from each other.\n",
    "* Files are as human-readable and human editable as possible (with exception of segmentation results' pieces and .npy images. unfortunately, .npy images are the only sane way to store arbitrary image data).\n",
    "* Everything is written in functional style, as much as reasonalbe for a python program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
